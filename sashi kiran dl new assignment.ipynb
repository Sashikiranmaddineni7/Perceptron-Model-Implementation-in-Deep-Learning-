{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805b7d5b",
   "metadata": {},
   "source": [
    "# HW1a The Perceptron (20 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2573523a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Download train.dat\n",
    "os.system(\"curl -O http://huang.eng.unt.edu/CSCE-5218/train.dat\")\n",
    "\n",
    "# Download test.dat\n",
    "os.system(\"curl -O http://huang.eng.unt.edu/CSCE-5218/test.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf50c5",
   "metadata": {},
   "source": [
    "# Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below. DO NOT import any other libraries to complete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a96f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    # Computing the dot product of two arrays\n",
    "    dot_prod = sum([array1[i] * array2[i] for i in range(len(array1))])\n",
    "    return dot_prod\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Computing the output of sigmoid function\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    # Computing the output of the model\n",
    "    return sigmoid(dot_product(weight, instance))\n",
    "\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    # Predict the label of an instance\n",
    "    return 1 if output(weights, instance) >= 0.5 else 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    # Initialize the weights to zero\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            # finding the input value to the sigmoid function\n",
    "            in_value = dot_product(weights, instance)\n",
    "            # finding the output of the sigmoid function\n",
    "            output = sigmoid(in_value)\n",
    "            # finding the error between the predicted output and the actual label\n",
    "            error = instance[-1] - output\n",
    "            # Update the weights\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc1701",
   "metadata": {},
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9420a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a285c26",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d997abf",
   "metadata": {},
   "source": [
    "In the first code snippet, we calculated the input value of the perceptron by taking the dot product of the weights and the input instance, and then we pass this input value through the sigmoid function to obtain the output value. We then calculate the error by subtracting the actual target value from the output value.\n",
    "\n",
    "In the second code snippet, we use the predict function to obtain the output value, but we do not apply the sigmoid function. Therefore, the error calculation would not be correct since the perceptron is not outputting a probability score. The error would be calculated based on the raw output value, which would not be in the range of 0 to 1.\n",
    "\n",
    "so we use the first code snippet to calculate the input and output values using the dot product and sigmoid function, respectively, in order to obtain the correct error calculation for the perceptron algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698704d",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475a6f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr:   5, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:   5, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "tr:  10, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:  10, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "tr:  10, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "tr:  25, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "tr:  25, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "tr:  25, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "tr:  25, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
      "tr:  25, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "tr:  25, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "tr:  50, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "tr:  50, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "tr:  50, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  50, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "tr:  50, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "tr:  50, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "tr:  50, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "tr:  50, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "tr:  50, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "tr:  50, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "tr:  75, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  75, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  75, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr:  75, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "tr:  75, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "tr:  75, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  75, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr:  75, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "tr:  75, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "tr:  75, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "tr:  75, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "tr:  75, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "tr:  75, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "tr:  75, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "tr:  75, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] \n",
    "num_epochs = [5, 10, 20, 50, 100]\n",
    "lr = [0.005, 0.01, 0.05]   \n",
    "\n",
    "for tr in tr_percent:\n",
    "    num_instances = int(len(instances_tr) * tr / 100)\n",
    "    instances = instances_tr[:num_instances]\n",
    "    for learning_rate in lr:\n",
    "        for epoch in num_epochs:\n",
    "            weights = train_perceptron(instances, learning_rate, epoch)\n",
    "            correct = 0\n",
    "            for instance in instances_te:\n",
    "                prediction = predict(weights, instance)\n",
    "                if prediction == instance[-1]:\n",
    "                    correct += 1      \n",
    "            accuracy = (correct / len(instances_te)) * 100\n",
    "            \n",
    "            print(\"tr: %3d, epochs: %3d, learning rate: %.3f; Accuracy (test, 100 instances): %.1f\" % (tr, epoch, learning_rate, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6afb7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf4437",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a257e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "-B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "\n",
    "tr: 100, epochs: 20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "tr: 200, epochs: 20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "\n",
    "-C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "-D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n",
    "3a. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "\n",
    "To get the best accuracy with the test dataset, it is not required to train using all of the training data. The test dataset is used to assess the performance of the trained model after it has been trained using the training dataset. If the model is correctly trained on a representative sample of the training data, it ought to be able to generalize effectively to new, unexplored data, including the test dataset.\n",
    "\n",
    "3b How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "\n",
    "Despite utilizing additional training data, there might be a number of reasons why the second run had worse accuracy than the first. The bigger training dataset could include more noise, outliers, or irrelevant samples, which the perceptron may have learnt to fit. This is one reason overfitting could be the cause. Unbalanced classes, subpar hyperparameters, or worse training data in the second run are some more potential causes.\n",
    "\n",
    "3c. Can you get higher accuracy with additional hyperparameters (higher than 80.0)?\n",
    "\n",
    "t's possible to achieve higher accuracy than 80.0 with additional hyperparameters, but there are no guarantees. The accuracy of a perceptron depends on several factors, including the quality and size of the training data, the complexity of the problem, the learning rate, and the number of epochs. Optimizing hyperparameters may lead to improved accuracy, but it requires careful experimentation and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e131dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb2a74",
   "metadata": {},
   "source": [
    "3d. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "No, it is not always efficient to train a perceptron for further epochs without taking other hyperparameters into account. More epochs of training can enhance performance, but it can also cause overfitting, when the perceptron becomes overly specialized to the training data and struggles on fresh, untrained input. Dependent on the task, data, and other hyperparameters, the ideal number of epochs is determined. It's critical to keep an eye on the training and validation errors and to halt training as soon as the perceptron exhibits signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
